{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this experiment we will study the following problem:  a lot of really good algorithms for matrix completion problem, that have a good quality of prediction, measured by RMSE metrics, have a poor quality of prediction, measured by special \"recommender metrics.\" We deside to analyse this quetsion and compare the quality of our algorithms, measured by different metrics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There by $HR_{10}$ and $HR_{20}$ we denote Hit Rate for top-10 and top-20 recommendations respectively.**\n",
    "\n",
    "**For estimating quality of our algoritms we will use 5-fold cross validation. But, to reduce compatational time we will choose best params, using not so big grid of parameters and will find the best pair, using only one fold. In general, of course, we sholud do a more exaustive grid search and use \"full\" cross-validation procedure. But in practice, expert articles said that the result obtained will also be good.**\n",
    "\n",
    "**We can tune approximation rank and regularization coefficient for FastALS and ALS respectively and approximation rank for Riemannian Optimization algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and provided functions\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import wget\n",
    "from io import StringIO \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "from  scipy.sparse import linalg\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "def get_movielens_1M_data(local_file=None):\n",
    "    '''Downloads movielens data and create Pandas DataFrame\n",
    "    '''\n",
    "    if not local_file:\n",
    "        print('Downloading data...')\n",
    "        zip_file_url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        zip_contents = wget.download(zip_file_url)\n",
    "        print('Done.')\n",
    "    else:\n",
    "        zip_contents = local_file\n",
    "    \n",
    "    print('Loading data into memory...')\n",
    "    with zipfile.ZipFile(zip_contents) as zfile:\n",
    "        zdata = zfile.read('ml-1m/ratings.dat').decode()\n",
    "        delimiter = ';'\n",
    "        zdata = zdata.replace('::', delimiter) # makes data compatible with pandas c-engine\n",
    "        ml_data = pd.read_csv(StringIO(zdata), sep=delimiter, header=None, engine='c',\n",
    "                                  names=['userid', 'movieid', 'rating', 'timestamp'],\n",
    "                                  usecols=['userid', 'movieid', 'rating'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Done.')\n",
    "    return ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse(ml_data, dtype=np.int64):\n",
    "    \"\"\"\n",
    "    Normalize data and make sparse matrix from pandas DataFrame\n",
    "    Args:\n",
    "        ml_data: (pandas DataFrame): Data Frame of users and ratings\n",
    "    Returns:\n",
    "        datamatrix (scipy.sparse): Sparse matrix (users - items).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize indices to avoid gaps\n",
    "    ml_data['movieid'] = ml_data.groupby('movieid', sort=False).grouper.group_info[0]\n",
    "    ml_data['userid'] = ml_data.groupby('userid', sort=False).grouper.group_info[0]\n",
    "    \n",
    "    # build sparse user-movie matrix\n",
    "    data_shape = ml_data[['userid', 'movieid']].max() + 1\n",
    "    data_matrix = sp.sparse.csr_matrix((ml_data['rating'],\n",
    "                                       (ml_data['userid'], ml_data['movieid'])),\n",
    "                                        shape=data_shape, dtype=dtype)\n",
    "    \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preproc(data, mode = 'hr', random_state=123):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (pandas DataFrame): data frame of initial data\n",
    "        mode (str): 'hr' or 'rmse'.\n",
    "    Returns:\n",
    "        ans (csr_matrix): csr_matrix of the data. \n",
    "        \n",
    "    if mode == 'hr', then all ratings with 4 and 5 goes to 1, else 0.\n",
    "    if mode == 'rmse', then ratings don't change.\n",
    "    \"\"\"\n",
    "    data_shuffle = shuffle(data, random_state=random_state)\n",
    "    \n",
    "    if (mode == 'hr'):\n",
    "        ans = data_shuffle[data_shuffle.rating >= 4].copy()\n",
    "        #ans.rating = 1\n",
    "        \n",
    "        ans.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    elif mode == 'rmse':\n",
    "        ans = data_shuffle.reset_index(drop=True, inplace=False)\n",
    "        \n",
    "    else:\n",
    "        return (\"Mistake. Please check input\")\n",
    "    \n",
    "    ans = make_sparse(ans)\n",
    "    \n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data = get_movielens_1M_data(local_file='ml-1m.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  movieid  rating\n",
       "0       1     1193       5\n",
       "1       1      661       3\n",
       "2       1      914       3\n",
       "3       1     3408       4\n",
       "4       1     2355       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6038x3533 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 575281 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse = data_preproc(data, mode='hr', random_state=123)\n",
    "data_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = data_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for working with data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hold_out(Arr, mode='hr', random_state=123, rmse_num = 3):\n",
    "    \"\"\"\n",
    "    Geberates hold out set for recommender problem:\n",
    "    Args:\n",
    "        Arr (np.array): \"Test\" array from which we will select items.\n",
    "        mode (str): 'hr' or 'rmse'. If 'hr' then get the top items for hold_out set. If 'rmse', \n",
    "        then perform random selection for hold_out set\n",
    "        random_state (int): random_state for reproducible results.\n",
    "        rmse_num (int): Number of random choised elements for hold_out set if mode='rmse'\n",
    "    Returns\n",
    "        hold_out (np.array): indices of hold_out elements. \n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    if mode == 'hr':\n",
    "        Arg_Arr = np.argsort(-Arr)\n",
    "        hold_out = Arg_Arr[:, 0]\n",
    "    elif mode == 'rmse':\n",
    "        hold_out = np.zeros((Arr.shape[0], rmse_num))\n",
    "        \n",
    "        for it, user in enumerate(Arr):\n",
    "            \n",
    "            hold_out[it] = np.random.choice(a = Arr.shape[1], size=rmse_num, replace=False)\n",
    "    \n",
    "    else:\n",
    "        return \"Mistake, check input!\"\n",
    "    \n",
    "    return hold_out[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(Arr, num):\n",
    "    \"\"\"\n",
    "    Get train and test sets\n",
    "    Args:\n",
    "        Arr (np.array): data_array\n",
    "        num (int): number of first elements to take\n",
    "    Returns:\n",
    "        X_train (no.array): train set\n",
    "        X_test (np.array): test set\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = data_array[:num]\n",
    "    X_test = data_array[num:]\n",
    "    \n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_train_test(X_train, X_test, hold_out):\n",
    "    \"\"\"\n",
    "    Check if the hold_out item does not lie in train set. If so, delete this user and assygn it to train set.\n",
    "    Args:\n",
    "        X_train (np.array): train set\n",
    "        X_test (np.array): test set\n",
    "        hold_out (np.array): hold_out indices\n",
    "    Returns:\n",
    "        X_train_copy (np.array): Changed copy of X_train in which we added test users, for which we find items,\n",
    "        that do not liy in training set\n",
    "        \n",
    "        X_test_copy (np.array): Changed copy of X_test from which we delete test users, for which we find items,\n",
    "        that do not liy in training set\n",
    "        \n",
    "        hold_out_copy (np.array): Changed copy of hold_out from which we delete rows, that corresponding \n",
    "        to test users, for which we find items, that do not liy in training set\n",
    "        \n",
    "    \"\"\"\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_test_copy = X_test.copy()\n",
    "    hold_out_copy = hold_out.copy()\n",
    "    \n",
    "    counter = 0\n",
    "    for it, user in enumerate(X_test):\n",
    "        if np.sum(X_train[:, hold_out[it]]) == 0:\n",
    "            X_test_copy = np.delete(X_test, it, 0)\n",
    "            hold_out_copy = np.delete(hold_out, it, 0)\n",
    "            X_train_copy = np.vstack([X_train_copy, user])\n",
    "            counter += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return (X_train_copy, X_test_copy, hold_out_copy, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommender_problem(train, test, hold_out):\n",
    "    \"\"\"\n",
    "    Zero hold_out elements in the test and create sparse matrix\n",
    "    Args:\n",
    "        train (np.array): train set\n",
    "        test (np.array): test set\n",
    "        hold_out (np.array): hold_out indices\n",
    "        \n",
    "    Returns:\n",
    "        problem (sparse.csr_matrix) full data matrix with zerod hold_out elements\n",
    "    \"\"\"\n",
    "    ind = np.arange(test.shape[0])[:, None]\n",
    "    \n",
    "    test_new = test.copy()\n",
    "    test_new[ind, hold_out] = 0\n",
    "    \n",
    "    train_sparse = sparse.csr_matrix(train)\n",
    "    test_sparse = sparse.csr_matrix(test_new)\n",
    "    \n",
    "    problem = sparse.vstack([train_sparse, test_sparse])\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_HR(X_test_pred, hold_out, top_k = 10):\n",
    "    \"\"\"\n",
    "    Calculate Hit Rate score. \n",
    "    Args:\n",
    "        X_test_pred (np.array): Predictions\n",
    "        hold_out (np.array): hold_out indices\n",
    "        top_k (int): number of top elements to choose for evaluating\n",
    "    Returns:\n",
    "        total_score (int): number of hits \n",
    "        total_score/N (float): hit rate\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    \n",
    "    for it, user in enumerate(X_test_pred):\n",
    "\n",
    "        user_argsort = np.argsort(-user)\n",
    "        total_score += np.intersect1d(hold_out[it], user_argsort[:top_k]).shape[0]\n",
    "\n",
    "    \n",
    "    return total_score, total_score/X_test_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(prediction, true, hold_out):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate RMSE score. \n",
    "    Args:\n",
    "        prediction (np.array): Predictions\n",
    "        true (np.array): true labels\n",
    "        hold_out (np.array): hold_out indices\n",
    "    Returns:\n",
    "        ans (float): RMSE score\n",
    "    \"\"\"\n",
    "    \n",
    "    ind = np.arange(true.shape[0])[:, None]   \n",
    "    \n",
    "    diff = prediction[ind, hold_out] - true[ind, hold_out]\n",
    "    ans = np.sqrt(np.mean(diff**2))\n",
    "    \n",
    "    return (ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from Algorithms import FastALS\n",
    "from Algorithms import RiemannianOptimization\n",
    "from Algorithms import ALS\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid_Search_1_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(data_array.shape[0]*0.8)\n",
    "X_train, X_test = get_train_test(data_array, num)\n",
    "    \n",
    "hold_out = generate_hold_out(X_test, mode='hr', random_state=123)\n",
    "(X_train, X_test, hold_out, counter) = change_train_test(X_train, X_test, hold_out)\n",
    "problem = get_recommender_problem(X_train, X_test, hold_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_1_fold(estimator, params, problem, X_test, hold_out, metric, top_k = None, debug_mode = False):\n",
    "    \"\"\"\n",
    "    Perfoms exhaustive grid search for parameters. \n",
    "    Find best parameters and calculte scores for the best parameters.\n",
    "    \n",
    "    Args:\n",
    "        estimator: estimator to fit\n",
    "        params (dic) dictionary of parameters\n",
    "        problem (csr_matrix): data with zerod hold_out set\n",
    "        X_test (np.array): test set\n",
    "        hold_out (np.array): hold_out indices\n",
    "        metric (str): 'hr' or 'rmse'.\n",
    "        top_k (int): top elements using for evaluating score.\n",
    "        debug_mode (bool): if True, print params and score on each iteration.\n",
    "    Returns:\n",
    "        best_score (float): best csore, obtained on best params\n",
    "        best_params (dic): best parameters, that was found by procedure\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    param_grid = list(ParameterGrid(params))\n",
    "    \n",
    "    best_params = []\n",
    "    \n",
    "    if metric == 'hr':\n",
    "        best_score = 0\n",
    "    \n",
    "    elif metric == 'rmse':\n",
    "        best_score = np.inf\n",
    "    \n",
    "    else:\n",
    "        return (\"Check metric.\")\n",
    "    \n",
    "    for param in param_grid:\n",
    "        try:\n",
    "            rank = param['rank']\n",
    "            estimator._rank = rank\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            max_iter = param['max_iter']\n",
    "            estimator._max_iter = max_iter\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            tol = param['tol']\n",
    "            estimator._tol = tol\n",
    "        except KeyError:\n",
    "            pass\n",
    "         \n",
    "        try:\n",
    "            reg_coef = param['reg_coef']\n",
    "            estimator._reg_coef = reg_coef\n",
    "        except KeyError:\n",
    "            pass    \n",
    "            \n",
    "        \n",
    "        estimator.fit(problem, trace=False, debug_mode=False)\n",
    "        \n",
    "        Pred = estimator.predict()\n",
    "        X_pred = Pred[X_train.shape[0]:]\n",
    "        \n",
    "        if metric == 'hr':\n",
    "            hits, cur_score = calculate_HR(X_pred, hold_out, top_k=top_k)\n",
    "            \n",
    "            if cur_score > best_score:\n",
    "                best_score = cur_score\n",
    "                best_params = param\n",
    "        \n",
    "        elif metric == 'rmse':\n",
    "            cur_score = calculate_RMSE(X_pred, X_test, hold_out)\n",
    "            \n",
    "            if cur_score < best_score:\n",
    "                best_score = cur_score\n",
    "                best_params = param\n",
    "            \n",
    "        else:\n",
    "            return(\"Check metric\")\n",
    "        \n",
    "        if cur_score < best_score:\n",
    "            best_score = cur_score\n",
    "            best_params = param\n",
    "        \n",
    "        if debug_mode:\n",
    "            print(\"rank: {0}, reg_coef: {1}, score: {2}\".format(param['rank'], param['reg_coef'], cur_score))\n",
    "                  \n",
    "    return (best_score, best_params)\n",
    "\n",
    "# Grid Search for FastALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for FastALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_als = FastALS.FastALS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dic = {'rank': np.linspace(5, 200, 20, dtype=int).tolist(), \n",
    "             'reg_coef': np.linspace(1, 100, 20, dtype=float).tolist(),\n",
    "             'max_iter': [1000], 'tol': [1e-5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_best_score_top_10, fast_best_params_top_10 = GridSearch_1_fold(fast_als, param_dic, problem, \n",
    "                                            X_test, hold_out, metric='hr', top_k = 10, debug_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_best_score_top_20, fast_best_params_top_20 = GridSearch_1_fold(fast_als, param_dic, problem, \n",
    "                                            X_test, hold_out, metric='hr', top_k = 20, debug_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Riman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "riman = RiemannianOptimization.RiemannianOptimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "riman_best_score_top_10, riman_best_params_top_10 = GridSearch_1_fold(riman, param_dic, problem, \n",
    "                                            X_test, hold_out, metric='hr', top_k = 10, debug_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "riman_best_score_top_20, riman_best_params_top_20 = GridSearch_1_fold(riman, param_dic, problem, \n",
    "                                            X_test, hold_out, metric='hr', top_k = 20, debug_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(cv, estimator, data_array, mode='hr', top_k = 10, random_state=123, debug_mode=False):\n",
    "    it = 1\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    data_array_shuffle = shuffle(data_array)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_index, test_index in cv.split(data_array_shuffle):\n",
    "        X_train, X_test = data_array_shuffle[train_index], data_array_shuffle[test_index]\n",
    "    \n",
    "        hold_out = generate_hold_out(X_test, mode=mode, random_state=123*(it+1))\n",
    "        (X_train, X_test, hold_out, counter) = change_train_test(X_train, X_test, hold_out)\n",
    "        problem = get_recommender_problem(X_train, X_test, hold_out)\n",
    "    \n",
    "        #fast_als = FastALS.FastALS(rank=approx_rank, max_iter=max_iter, tol=tol, reg_coef=reg_coef)\n",
    "        estimator.fit(problem, trace=False, debug_mode=False)\n",
    "    \n",
    "        Pred = estimator.predict()\n",
    "        X_pred = Pred[X_train.shape[0]:]\n",
    "        \n",
    "        if mode == 'hr':\n",
    "            cur_score = calculate_HR(X_pred, hold_out, top_k=top_k)\n",
    "        elif mode == 'rmse':\n",
    "            cur_score = calculate_RMSE(X_pred, X_test, hold_out)\n",
    "        \n",
    "        if debug_mode == True:\n",
    "            print(\"It:\", it, \"cur_score:\", cur_score)\n",
    "    \n",
    "        scores.append(cur_score)\n",
    "    \n",
    "    it += 1\n",
    "    \n",
    "    return (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross_val for FastALS top10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "max_iter = 2000\n",
    "tol = 1e-5\n",
    "approx_rank = fast_best_params_top_10['rank']\n",
    "reg_coef = fast_best_params_top_10['reg_coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = FastALS.FastALS(max_iter=max_iter, tol=tol, rank=approx_rank, reg_coef=reg_coef)\n",
    "fast_scores_top_10 = cross_validation(kf, estimator, data_array, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross_val for FastALS top10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "\n",
    "max_iter = 2000\n",
    "tol = 1e-5\n",
    "approx_rank = fast_best_params_top_20['rank']\n",
    "reg_coef = fast_best_params_top_20['reg_coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = FastALS.FastALS(max_iter=max_iter, tol=tol, rank=approx_rank, reg_coef=reg_coef)\n",
    "fast_scores_top_20 = cross_validation(kf, estimator, data_array, top_k = top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross_val for Riman top10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "\n",
    "max_iter = 2000\n",
    "tol = 1e-5\n",
    "approx_rank = riman_best_params_top_10['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RiemannianOptimization.RiemannianOptimization(max_iter=max_iter, tol=tol, \n",
    "                                                          rank=approx_rank, reg_coef=reg_coef)\n",
    "riman_scores_top_10 = cross_validation(kf, estimator, data_array, top_k = top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "\n",
    "max_iter = 2000\n",
    "tol = 1e-5\n",
    "approx_rank = riman_best_params_top_20['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RiemannianOptimization.RiemannianOptimization(max_iter=max_iter, tol=tol, \n",
    "                                                          rank=approx_rank, reg_coef=reg_coef)\n",
    "riman_scores_top_20 = cross_validation(kf, estimator, data_array, top_k = top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_als_benchmarks = pd.DataFrame({\"HR_10\": [np.mean(fast_scores_top_10), np.std(fast_scores_top_10)], \n",
    "                               \"HR_20\": [np.mean(fast_scores_top_20), np.mean(fast_scores_top_20)]}, \n",
    "                                   index = ['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "riman_benchmarks = pd.DataFrame({\"HR_10\": [np.mean(riman_scores_top_10), np.std(riman_scores_top_10)], \n",
    "                               \"HR_20\": [np.mean(riman_scores_top_20), np.mean(riman_scores_top_20)]}, \n",
    "                                   index = ['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
